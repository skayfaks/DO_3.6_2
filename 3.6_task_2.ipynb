{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Генерация синтетических данных с использованием Apache Spark\n",
    "\n",
    "Цель задания: Использовать Apache Spark для создания синтетического набора данных, который имитирует информацию о покупках в интернет-магазине. \n",
    "- Набор данных должен включать в себя информацию о заказах, включая дату заказа, идентификатор пользователя, название товара, количество и цену. Сгенерированные данные будут использованы для последующего анализа покупательской активности и понимания потребительских трендов.\n",
    "\n",
    "Шаги выполнения:\n",
    "1. Генерация данных:\n",
    "\n",
    "- Создать DataFrame с полями: Дата, UserID, Продукт, Количество, Цена.\n",
    "\n",
    "- Данные для поля Продукт генерируются из списка возможных товаров ( не меньше 5 товаров )\n",
    "\n",
    "- Количество и Цена должны генерироваться случайно в заданных пределах.\n",
    "\n",
    "- Дата должна быть в пределах последнего года.\n",
    "\n",
    "- UserID представляет собой случайное число, имитирующее идентификаторы пользователей.\n",
    "\n",
    "- Обратите внимание, что должна быть возможности изменять количество сгенерированных строк. Минимальное количество - 1000 строк.\n",
    "\n",
    "2. Сохранение данных:\n",
    "\n",
    "- Сохранить сгенерированный DataFrame в формате CSV для последующего анализа.\n",
    "\n",
    "- Результат выполнения задания необходимо выложить в github/gitlab и указать ссылку на Ваш репозиторий (не забудьте: репозиторий должен быть публичным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anton/.local/lib/python3.10/site-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from pyspark.sql.functions import rand, col\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/30 13:27:30 WARN Utils: Your hostname, ubuntu-prox resolves to a loopback address: 127.0.1.1; using 192.168.1.30 instead (on interface enp6s18)\n",
      "24/06/30 13:27:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/30 13:27:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"task_3.6_2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "# df = spark.read.csv(\"/tmp/resources/zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Генерация данных:\n",
    "\n",
    "- Создать DataFrame с полями: Дата, UserID, Продукт, Количество, Цена.\n",
    "\n",
    "- Данные для поля Продукт генерируются из списка возможных товаров ( не меньше 5 товаров )\n",
    "\n",
    "- Количество и Цена должны генерироваться случайно в заданных пределах.\n",
    "\n",
    "- Дата должна быть в пределах последнего года.\n",
    "\n",
    "- UserID представляет собой случайное число, имитирующее идентификаторы пользователей.\n",
    "\n",
    "- Обратите внимание, что должна быть возможности изменять количество сгенерированных строк. Минимальное количество - 1000 строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 1000\n",
    "products = [\"Молоко\", \"Ноутбук\", \"Книга\", \"Сноу-Борд\", \"Кружка\"]\n",
    "min_quantity = 1\n",
    "max_quantity = 10\n",
    "min_price = 10.0\n",
    "max_price = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать DataFrame с полями: Дата, UserID, Продукт, Количество, Цена.\n",
    "\n",
    "data = [(datetime.now() - timedelta(days=random.randint(0, 365)), # Дата должна быть в пределах последнего года (365 дней).\n",
    "        random.randint(1, 1000), # UserID представляет собой случайное число, имитирующее идентификаторы пользователей.\n",
    "        random.choice(products), # Продукт. Данные для поля Продукт генерируются из списка возможных товаров ( не меньше 5 товаров )\n",
    "        random.randint(min_quantity, max_quantity), # Количество. Генерируется случайно в заданных пределах min_quantity-max_quantity.\n",
    "        round(random.uniform(min_price, max_price),2)) for _ in range(num_records)] # Цена. Генерируется случайно в заданных пределах min_price-max_price.\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Дата\", \"UserID\", \"Продукт\", \"Количество\", \"Цена\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Сохранение данных:\n",
    "\n",
    "- Сохранить сгенерированный DataFrame в формате CSV для последующего анализа.\n",
    "\n",
    "- Результат выполнения задания необходимо выложить в github/gitlab и указать ссылку на Ваш репозиторий (не забудьте: репозиторий должен быть публичным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+----------+-----+\n",
      "|                Дата|UserID|  Продукт|Количество| Цена|\n",
      "+--------------------+------+---------+----------+-----+\n",
      "|2023-10-01 13:27:...|   550|Сноу-Борд|         8|12.92|\n",
      "|2024-01-01 13:27:...|   815|Сноу-Борд|         6|25.94|\n",
      "|2023-11-18 13:27:...|   313|    Книга|         4|21.39|\n",
      "|2023-12-01 13:27:...|   950|   Кружка|         3|52.92|\n",
      "|2024-01-25 13:27:...|   518|    Книга|         6|56.83|\n",
      "|2024-05-18 13:27:...|   461|    Книга|         9|43.67|\n",
      "|2024-06-03 13:27:...|   450|  Ноутбук|         3|22.46|\n",
      "|2023-10-01 13:27:...|   733|   Кружка|         5|94.72|\n",
      "|2024-06-26 13:27:...|   626|Сноу-Борд|         3|76.14|\n",
      "|2023-10-21 13:27:...|   801|   Кружка|         6|10.94|\n",
      "+--------------------+------+---------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-write-dataframe-to-csv-file/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создается папка 3.6_2.csv в ней файлы экспорта\n",
    "df.coalesce(1) \\\n",
    "  .write \\\n",
    "  .format(\"csv\") \\\n",
    "  .options(header='True', delimiter=',') \\\n",
    "  .mode('overwrite') \\\n",
    "  .save(\"3.6_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write \\\n",
    "#     .format(\"csv\") \\\n",
    "#     .options(header='True', delimiter=',') \\\n",
    "#     .mode('overwrite') \\\n",
    "#     .save(\"3.6_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = \"data.csv\"\n",
    "# df.write \\\n",
    "#         .options(header='True', delimiter=',') \\\n",
    "#         .mode('overwrite') \\\n",
    "#         .csv(csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
